{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc9b4dc-99de-4f18-892f-6dc59115ed5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (1.26.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (1.13.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: gensim in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (4.3.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pyLDAvis in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (3.4.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: click in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from pyLDAvis) (2.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: numexpr in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from pyLDAvis) (2.10.1)\n",
      "Requirement already satisfied: funcy in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python311\\lib\\site-packages (from pyLDAvis) (65.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: wrapt in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->pyLDAvis) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy scipy scikit-learn gensim nltk pyLDAvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee5ed6f-0457-44ac-8519-1397c87704de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Collection\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d79c391-c27c-42bd-ac76-40a5678344f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "documents = newsgroups.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd72c676-0c3d-4f5d-914f-9374151a2013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Text Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce5684d2-2c2a-4567-8693-bf4561160229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords and wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7b80d59-911d-48e3-bcdd-5c06bb7d3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83946692-dc25-4db0-a017-2db56d786730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Remove punctuation, numbers, and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize text\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84083adf-24bd-4ae3-90a6-d48696049139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all documents\n",
    "preprocessed_docs = [preprocess(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ae183-f94f-4c59-a1b6-8cf07fcdf1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Topic Modeling with LDA\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Tokenize preprocessed documents\n",
    "tokenized_docs = [doc.split() for doc in preprocessed_docs]\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "\n",
    "# Filter out extreme values\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "\n",
    "# Create a document-term matrix\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# Apply LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx} \\nWords: {topic}\")\n",
    "\n",
    "# Visualize the topics using pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# Prepare visualization\n",
    "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05b1d756-6957-4348-925b-84f5ce6a96f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000001  0.5529127  0.70223314 0.54379404 0.5820057 ]\n",
      " [0.5529127  0.9999999  0.36349177 0.7936087  0.83862364]\n",
      " [0.70223314 0.36349177 1.         0.51533675 0.5179916 ]\n",
      " [0.54379404 0.7936087  0.51533675 1.0000001  0.8742099 ]\n",
      " [0.5820057  0.83862364 0.5179916  0.8742099  0.9999999 ]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Word Embeddings and Similarity Measurement\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Get document vectors by averaging word vectors\n",
    "def document_vector(doc):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in w2v_model.wv.key_to_index]\n",
    "    # Average the vectors of all words in the document\n",
    "    return np.mean(w2v_model.wv[doc], axis=0)\n",
    "\n",
    "# Compute document vectors\n",
    "doc_vectors = np.array([document_vector(doc) for doc in tokenized_docs])\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(doc_vectors)\n",
    "\n",
    "# Display similarity between first 5 documents\n",
    "print(similarity_matrix[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb758e71-e2dd-4801-8abe-84791866f0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Words in Each Topic:\n",
      "Topic 0: ['x', 'file', 'window', 'image', 'program', 'use', 'version', 'application', 'available', 'server']\n",
      "Topic 1: ['space', 'armenian', 'new', 'turkish', 'center', 'year', 'earth', 'u', 'research', 'may']\n",
      "Topic 2: ['israel', 'jew', 'israeli', 'arab', 'muslim', 'jewish', 'article', 'right', 'university', 'state']\n",
      "Topic 3: ['people', 'would', 'one', 'gun', 'dont', 'said', 'u', 'think', 'right', 'know']\n",
      "Topic 4: ['key', 'government', 'would', 'law', 'use', 'system', 'chip', 'public', 'one', 'u']\n",
      "Topic 5: ['car', 'article', 'like', 'one', 'get', 'nntppostinghost', 'would', 'dont', 'im', 'good']\n",
      "Topic 6: ['university', 'nntppostinghost', 'drive', 'system', 'thanks', 'would', 'know', 'email', 'card', 'computer']\n",
      "Topic 7: ['god', 'one', 'people', 'would', 'say', 'christian', 'think', 'dont', 'know', 'article']\n",
      "Topic 8: ['would', 'one', 'article', 'use', 'b', 'like', 'dont', 'get', 'time', 'also']\n",
      "Topic 9: ['game', 'team', 'year', 'player', 'university', 'article', 'would', 'go', 'play', 'nntppostinghost']\n"
     ]
    }
   ],
   "source": [
    "# Extracting the list of words in each topic\n",
    "topic_words = lda_model.show_topics(num_topics=10, num_words=10, formatted=False)\n",
    "topics = {}\n",
    "for topic in topic_words:\n",
    "    topic_id, words = topic\n",
    "    topics[topic_id] = [word for word, _ in words]\n",
    "\n",
    "print(\"List of Words in Each Topic:\")\n",
    "for topic_id, words in topics.items():\n",
    "    print(f\"Topic {topic_id}: {words}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45bd39a-a82f-449a-91e6-8adfff3bd52e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Convert LDA topic distribution for each document into a NumPy array\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m lda_vectors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([lda_model\u001b[38;5;241m.\u001b[39mget_document_topics(doc, minimum_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcorpus\u001b[49m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create a matrix of topic probabilities\u001b[39;00m\n\u001b[0;32m      9\u001b[0m lda_vectors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[topic_prob[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m topic_prob \u001b[38;5;129;01min\u001b[39;00m doc] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m lda_vectors])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert LDA topic distribution for each document into a NumPy array\n",
    "lda_vectors = np.array([lda_model.get_document_topics(doc, minimum_probability=0) for doc in corpus])\n",
    "\n",
    "# Create a matrix of topic probabilities\n",
    "lda_vectors = np.array([[topic_prob[1] for topic_prob in doc] for doc in lda_vectors])\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "lda_tsne = tsne_model.fit_transform(lda_vectors)\n",
    "\n",
    "# Plot t-SNE results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lda_tsne[:, 0], lda_tsne[:, 1], alpha=0.7, s=60)\n",
    "plt.title('t-SNE Visualization of Documents (LDA Vectors)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e830e2-bd09-4f90-bddf-b1bb92d66715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Comparison of Document Similarity\n",
    "# Similarity using LDA vectors\n",
    "lda_similarity_matrix = cosine_similarity(lda_vectors)\n",
    "\n",
    "print(\"Cosine Similarity Matrix using LDA Vectors (first 5 documents):\")\n",
    "print(lda_similarity_matrix[:5, :5])\n",
    "\n",
    "# Similarity using Word Embeddings\n",
    "embedding_similarity_matrix = cosine_similarity(doc_vectors)\n",
    "\n",
    "print(\"Cosine Similarity Matrix using Word Embeddings (first 5 documents):\")\n",
    "print(embedding_similarity_matrix[:5, :5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
